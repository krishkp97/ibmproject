!pip install pyspark==3.1.2 -q
!pip install findspark -q

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# FindSpark simplifies the process of using Apache Spark with Python

import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.pipeline import PipelineModel
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import StandardScaler

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/NASA_airfoil_noise_raw.csv

df = spark.read.csv("NASA_airfoil_noise_raw.csv", header=True, inferSchema=True)

#your code goes here
rowcount1 = df.count()
print(rowcount1)

df = df.dropDuplicates()

#your code goes here

rowcount2 = df.count()
print(rowcount2)

df = df.dropna()

#your code goes here

rowcount3 = df.count()
print(rowcount3)

df = df.withColumnRenamed("SoundLevel","SoundLevelDecibels")

# your code goes here
df.write.mode("overwrite").parquet("NASA_airfoil_noise_cleaned.parquet")


print("Part 1 - Evaluation")

print("Total rows = ", rowcount1)
print("Total rows after dropping duplicate rows = ", rowcount2)
print("Total rows after dropping duplicate rows and rows with null values = ", rowcount3)
print("New column name = ", df.columns[-1])

import os

print("NASA_airfoil_noise_cleaned.parquet exists :", os.path.isdir("NASA_airfoil_noise_cleaned.parquet"))

#your code goes here

df = spark.read.parquet("NASA_airfoil_noise_cleaned.parquet")

#your code goes here

rowcount4 = df.count()
print(rowcount4)

#your code goes here
assembler = VectorAssembler(inputCols=['Frequency','AngleOfAttack','ChordLength','FreeStreamVelocity','SuctionSideDisplacement'], outputCol="features")

#your code goes here

scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")

#your code goes here

lr = LinearRegression(featuresCol="scaledFeatures", labelCol="SoundLevelDecibels")

#your code goes here

pipeline = Pipeline(stages=[assembler, scaler, lr])

# Split the data into training and testing sets with 70:30 split.
# set the value of seed to 42
# the above step is very important. DO NOT set the value of seed to any other value other than 42.

#your code goes here

(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)


# Fit the pipeline using the training data
# your code goes here

pipelineModel = pipeline.fit(trainingData)

print("Part 2 - Evaluation")
print("Total rows = ", rowcount4)
ps = [str(x).split("_")[0] for x in pipeline.getStages()]

print("Pipeline Stage 1 = ", ps[0])
print("Pipeline Stage 2 = ", ps[1])
print("Pipeline Stage 3 = ", ps[2])

print("Label column = ", lr.getLabelCol())

# Make predictions on testing data
# your code goes here

predictions = pipelineModel.transform(testingData)

from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="SoundLevelDecibels", metricName="mse")
mse = evaluator.evaluate(predictions)
print(mse)

evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="SoundLevelDecibels", metricName="mae")
mae = evaluator.evaluate(predictions)
print(mae)

evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="SoundLevelDecibels", metricName="r2")
r2 = evaluator.evaluate(predictions)
print(r2)

print("Part 3 - Evaluation")

print("Mean Squared Error = ", round(mse,2))
print("Mean Absolute Error = ", round(mae,2))
print("R Squared = ", round(r2,2))

lrModel = pipelineModel.stages[-1]

print("Intercept = ", round(lrModel.intercept,2))


# Save the pipeline model as "Final_Project"
# your code goes here
pipelineModel.write().save("Final_Project")

# Load the pipeline model you have created in the previous step
loadedPipelineModel = PipelineModel.load("Final_Project")

# Use the loaded pipeline model and make predictions using testingData
predictions = loadedPipelineModel.transform(testingData)

#show top 5 rows from the predections dataframe. Display only the label column and predictions
#your code goes here
predictions.select("SoundLevelDecibels","prediction").show()

print("Part 4 - Evaluation")

loadedmodel = loadedPipelineModel.stages[-1]
totalstages = len(loadedPipelineModel.stages)
inputcolumns = loadedPipelineModel.stages[0].getInputCols()

print("Number of stages in the pipeline = ", totalstages)
for i,j in zip(inputcolumns, loadedmodel.coefficients):
    print(f"Coefficient for {i} is {round(j,4)}")

    
    